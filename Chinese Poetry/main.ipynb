{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as f\n",
    "import _pickle as cPi\n",
    "import random\n",
    "\n",
    "from data import *\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254225\n"
     ]
    }
   ],
   "source": [
    "####### prepare data #######\n",
    "poems_all = poem_loader()\n",
    "#data_t = poems[0:3]\n",
    "#for s in data_t:\n",
    "    #print(s)\n",
    "\n",
    "# dictionary for words\n",
    "word_to_idx = {}\n",
    "for poem in poems_all:\n",
    "    for word in poem:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "word_to_idx['<EOP>'] = len(word_to_idx)\n",
    "word_to_idx['<START>'] = len(word_to_idx)\n",
    "\n",
    "#with open('wordDic', 'wb') as file:\n",
    "    #cPi.dump(word_to_idx, file)\n",
    "\n",
    "#print(len(word_to_idx))\n",
    "print(len(poems_all))\n",
    "poems = random.sample(poems_all, 2560)\n",
    "\n",
    "# change poem sentences into lists\n",
    "# poems is a list of list\n",
    "for i in range(len(poems)):\n",
    "    poems[i] = toList(poems[i])\n",
    "    poems[i].append(\"<EOP>\")\n",
    "#print(poems[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class PoetryModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers):\n",
    "        super(PoetryModel, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers) \n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        length = x.size()[0]\n",
    "        embeds = self.embed(x).view((length, 1, -1))\n",
    "        out, (h, c) = self.lstm(embeds, h)\n",
    "        # 在最后分类器的地方使用非线性\n",
    "        out = f.relu(self.linear(out.view(length, -1)))\n",
    "        \n",
    "        return out, (h, c)\n",
    "    \n",
    "    # 包括 h 的initial和 cell 的initial    \n",
    "    def hidden_initial(self, length=1):\n",
    "        return (torch.zeros(length, 1, self.hidden_dim),\n",
    "                torch.zeros(length, 1, self.hidden_dim))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/8], Batch:10, Loss:7.3877\n",
      "Epoch[1/8], Batch:20, Loss:7.2987\n",
      "Epoch[1/8], Batch:30, Loss:7.1663\n",
      "Epoch[1/8], Batch:40, Loss:7.1971\n",
      "Epoch[2/8], Batch:10, Loss:7.0004\n",
      "Epoch[2/8], Batch:20, Loss:7.0375\n",
      "Epoch[2/8], Batch:30, Loss:6.9349\n",
      "Epoch[2/8], Batch:40, Loss:6.9830\n",
      "Epoch[3/8], Batch:10, Loss:6.7754\n",
      "Epoch[3/8], Batch:20, Loss:6.8300\n",
      "Epoch[3/8], Batch:30, Loss:6.7134\n",
      "Epoch[3/8], Batch:40, Loss:6.7984\n",
      "Epoch[4/8], Batch:10, Loss:6.6064\n",
      "Epoch[4/8], Batch:20, Loss:6.6797\n",
      "Epoch[4/8], Batch:30, Loss:6.5872\n",
      "Epoch[4/8], Batch:40, Loss:6.6701\n",
      "Epoch[5/8], Batch:10, Loss:6.4892\n",
      "Epoch[5/8], Batch:20, Loss:6.5677\n",
      "Epoch[5/8], Batch:30, Loss:6.5005\n",
      "Epoch[5/8], Batch:40, Loss:6.5874\n",
      "Epoch[6/8], Batch:10, Loss:6.3949\n",
      "Epoch[6/8], Batch:20, Loss:6.4755\n",
      "Epoch[6/8], Batch:30, Loss:6.4027\n",
      "Epoch[6/8], Batch:40, Loss:6.5021\n",
      "Epoch[7/8], Batch:10, Loss:6.3036\n",
      "Epoch[7/8], Batch:20, Loss:6.3821\n",
      "Epoch[7/8], Batch:30, Loss:6.3232\n",
      "Epoch[7/8], Batch:40, Loss:6.4101\n",
      "Epoch[8/8], Batch:10, Loss:6.2082\n",
      "Epoch[8/8], Batch:20, Loss:6.2816\n",
      "Epoch[8/8], Batch:30, Loss:6.2269\n",
      "Epoch[8/8], Batch:40, Loss:6.3269\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "# Hyper-parameters\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 8\n",
    "batch_size = 64\n",
    "vocab_size = len(word_to_idx)\n",
    "embed_dim = 256\n",
    "hidden_dim = 1024\n",
    "num_layers = 1\n",
    "\n",
    "model = PoetryModel(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "TRAINSIZE = len(poems)\n",
    "for epoch in range(num_epochs):\n",
    "    for batchIndex in range(TRAINSIZE // batch_size):\n",
    "        model.zero_grad()\n",
    "        loss = 0\n",
    "        counts = 0\n",
    "        for i in range(batchIndex * batch_size, min((batchIndex + 1) * batch_size, TRAINSIZE)):\n",
    "            s = poems[i]\n",
    "            x, o = in_and_out(s, word_to_idx)\n",
    "            hidden = model.hidden_initial()\n",
    "            output, hidden = model(x, hidden)\n",
    "            loss += criterion(output, o)\n",
    "            counts +=1\n",
    "        loss = loss / counts\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batchIndex + 1) % 10 == 0:\n",
    "            print ('Epoch[{}/{}], Batch:{}, Loss:{:.4f}'.format(epoch+1, num_epochs,batchIndex+1, loss.item()))\n",
    "\n",
    "torch.save(model.state_dict(), 'PoetryModel.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雨暗無人間，不見不可。\n",
      "一株風雨一聲，一點不可憐。\n",
      "昊東風流水，不見不可。\n",
      "夜凉人間不可憐，一點不見不可憐。\n"
     ]
    }
   ],
   "source": [
    "# generating\n",
    "\n",
    "model_test = PoetryModel(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "model_test.load_state_dict(torch.load('PoetryModel.ckpt'))\n",
    "\n",
    "word_to_idx = cPi.load(open('./data_set/wordDic', 'br'))\n",
    "idx_to_word = {value: key for key, value in word_to_idx.items()}\n",
    "max_length = 50\n",
    "\n",
    "def PoetryGenerator(startword = '<START>'):\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        out_poem = ''\n",
    "        x = torch.tensor([word_to_idx[startword]])\n",
    "        hidden = model_test.hidden_initial()\n",
    "        if (startword != '<START>'):\n",
    "            out_poem = startword\n",
    "        for i in range(max_length):\n",
    "            out, hidden = model_test(x, hidden)\n",
    "            topv, topi = out.data.topk(1)\n",
    "            idx = topi[0][0]\n",
    "            w = idx_to_word[idx.item()]\n",
    "            if w == '<EOP>' :\n",
    "                break\n",
    "            else:\n",
    "                out_poem += w\n",
    "            x = torch.tensor([word_to_idx[w]])\n",
    "    return out_poem\n",
    "        \n",
    "\n",
    "print (PoetryGenerator('雨'))\n",
    "print (PoetryGenerator('一'))\n",
    "print (PoetryGenerator('昊'))\n",
    "print (PoetryGenerator('夜'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 more epochs   \n",
    "Since the second parts of the sentences didn't make any sense,and the loss above continuosly decreased,   \n",
    "I trained the model on 4 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/4], Batch:10, Loss:6.1919\n",
      "Epoch[1/4], Batch:20, Loss:6.2261\n",
      "Epoch[1/4], Batch:30, Loss:6.1789\n",
      "Epoch[1/4], Batch:40, Loss:6.3158\n",
      "Epoch[2/4], Batch:10, Loss:6.0517\n",
      "Epoch[2/4], Batch:20, Loss:6.1191\n",
      "Epoch[2/4], Batch:30, Loss:6.0789\n",
      "Epoch[2/4], Batch:40, Loss:6.2227\n",
      "Epoch[3/4], Batch:10, Loss:5.9651\n",
      "Epoch[3/4], Batch:20, Loss:6.0366\n",
      "Epoch[3/4], Batch:30, Loss:5.9952\n",
      "Epoch[3/4], Batch:40, Loss:6.1391\n",
      "Epoch[4/4], Batch:10, Loss:5.8850\n",
      "Epoch[4/4], Batch:20, Loss:5.9484\n",
      "Epoch[4/4], Batch:30, Loss:5.9128\n",
      "Epoch[4/4], Batch:40, Loss:6.0594\n"
     ]
    }
   ],
   "source": [
    "learning_rate_c = 1e-3\n",
    "num_epochs_c = 4\n",
    "batch_size_c = 64\n",
    "vocab_size_c = len(word_to_idx)\n",
    "embed_dim_c = 256\n",
    "hidden_dim_c = 1024\n",
    "num_layers_c = 1\n",
    "\n",
    "model_c = PoetryModel(vocab_size_c, embed_dim_c, hidden_dim_c, num_layers_c)\n",
    "model_c.load_state_dict(torch.load('PoetryModel.ckpt'))\n",
    "criterion_c = nn.CrossEntropyLoss()\n",
    "optimizer_c = optim.Adam(model_c.parameters(), lr=learning_rate_c)\n",
    "\n",
    "TRAINSIZE = len(poems)\n",
    "for epoch in range(num_epochs_c):\n",
    "    for batchIndex in range(TRAINSIZE // batch_size_c):\n",
    "        model_c.zero_grad()\n",
    "        loss_c = 0\n",
    "        counts_c = 0\n",
    "        for i in range(batchIndex * batch_size_c, min((batchIndex + 1) * batch_size_c, TRAINSIZE)):\n",
    "            s = poems[i]\n",
    "            x, o = in_and_out(s, word_to_idx)\n",
    "            hidden = model_c.hidden_initial()\n",
    "            output, hidden = model_c(x, hidden)\n",
    "            loss_c += criterion(output, o)\n",
    "            counts_c +=1\n",
    "        loss_c = loss_c / counts_c\n",
    "        loss_c.backward()\n",
    "        optimizer_c.step()\n",
    "        if (batchIndex + 1) % 10 == 0:\n",
    "            print ('Epoch[{}/{}], Batch:{}, Loss:{:.4f}'.format(epoch+1, num_epochs_c, batchIndex+1, loss_c.item())) \n",
    "            \n",
    "torch.save(model_c.state_dict(), 'PoetryModel_c.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "雨暗無人管，不知此日之。\n",
      "一徑山水抱城流，一點春風雨露香。\n",
      "林木蕭條一點春，一時來時有餘生。\n",
      "翊風吹雨過，天下有新詩。\n"
     ]
    }
   ],
   "source": [
    "model_c_test = PoetryModel(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "model_c_test.load_state_dict(torch.load('PoetryModel_c.ckpt'))\n",
    "\n",
    "def PoetryGenerator2(startword = '<START>'):\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        out_poem = ''\n",
    "        x = torch.tensor([word_to_idx[startword]])\n",
    "        hidden = model_c_test.hidden_initial()\n",
    "        if (startword != '<START>'):\n",
    "            out_poem = startword\n",
    "        for i in range(max_length):\n",
    "            out, hidden = model_c_test(x, hidden)\n",
    "            topv, topi = out.data.topk(1)\n",
    "            idx = topi[0][0]\n",
    "            w = idx_to_word[idx.item()]\n",
    "            if w == '<EOP>' :\n",
    "                break\n",
    "            else:\n",
    "                out_poem += w\n",
    "            x = torch.tensor([word_to_idx[w]])\n",
    "    return out_poem\n",
    "        \n",
    "print (PoetryGenerator2('雨'))\n",
    "print (PoetryGenerator2('一'))\n",
    "print (PoetryGenerator2('林'))\n",
    "print (PoetryGenerator2('翊'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The result seems much better!   \n",
    "As the loss still seems to decrease, you can continue training on more epochs and see if a better result can be obtained.      \n",
    "It could be a little bit time-consuming if you run on cpu(about 1h20min for one epoch :( )     \n",
    "The last character of my name is \"翊\"(which can represent the sound of birds flying)，and the last sentence says that when it(\"翊风\") blows and rains, it brings new poems.     \n",
    "It seems that the machine even learns to know who wrote the poem generator program! Just kidding :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
